{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Word Embeddings Theory.ipynb","provenance":[{"file_id":"1UBWBHxQB5W2L5eab5xIZbpGa5mjOfARI","timestamp":1613406076975}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"v15ArteeyCTd"},"source":["This tutorial introduces word embeddings\n","\n","It contains complete code to train word embeddings from scratch on a small dataset"]},{"cell_type":"markdown","metadata":{"id":"Fpbp5X80x37W"},"source":["# 1) Representing text as numbers"]},{"cell_type":"markdown","metadata":{"id":"6zrOuNdUx_0B"},"source":["Machine learning models take vectors (arrays of numbers) as input\n","\n","When working with text, the first thing we must do come up with a strategy to convert strings to numbers (or to \"vectorize\" the text) before feeding it to the model\n","\n","In this section, we will look at three strategies for doing so\n","\n","1. One-hot encodings\n","2. Encode each word with a unique number\n","3. Word embeddings"]},{"cell_type":"markdown","metadata":{"id":"3GBYmzHjf-LF"},"source":["\r\n","\r\n","---\r\n","\r\n","\r\n","\r\n","**Frequency based Embedding**\r\n","1.   Count Vector\r\n","2.   TF-IDF Vector\r\n","3.   Co-Occurrence Vector\r\n","\r\n","**Prediction based Embedding**\r\n","1.   CBOW (Bag of Words)\r\n","2.   Skip-Gran"]},{"cell_type":"markdown","metadata":{"id":"xjG71dUoy_AF"},"source":["## 1) One-hot encodings"]},{"cell_type":"markdown","metadata":{"id":"Za5zVTXSzGXt"},"source":["We might \"one-hot\" encode each word in our vocabulary\n","\n","Consider the sentence **\"The cat sat on the mat\"**\n","\n","The vocabulary (unique words) in this sentence is (cat, sat, on, the, mat)\n","\n","To represent each word, we will create a zero vector with length = vocabulary, then place a one in index that corresponds to the word"]},{"cell_type":"markdown","metadata":{"id":"9C_JeSla6IKd"},"source":["![alt text](https://drive.google.com/uc?id=1MbwyW_fPcqydSSRfqThyPi8xuqjJKHkf)"]},{"cell_type":"markdown","metadata":{"id":"iaaPf1zV6ogn"},"source":["To create a vector that contains the encoding of sentence, then we can concatenate the one-hot vectors for each word"]},{"cell_type":"markdown","metadata":{"id":"zGFhGTgU7oMo"},"source":["**This approach is inefficient**\n","\n","A one-hot encoded vector is sparse (most indices are zero)\n","\n","Imagine we have 10,000 words in the vocabulary, then to one-hot encode each word, we have to create a vector where 99.99% of the elements are zero"]},{"cell_type":"markdown","metadata":{"id":"eY4AZfi076XG"},"source":["## 2) Encode each word with a unique number"]},{"cell_type":"markdown","metadata":{"id":"ohv8F5DKD8tD"},"source":["**Second approach: Encode each word with a unique number**\n","\n","We can encode each word using a unique number, In above example, \"The cat sat on the mat\"\n","\n","We could assign 1 to \"the\", 2 to \"sat\" and so on, Then the encoded sentence would be, \n","\"The cat sat on the mat\" be like [1, 2, 3, 4, 1, 6]\n","\n","This appoach is efficient than the sparse vector appoach"]},{"cell_type":"markdown","metadata":{"id":"kq22Bv0LE9gU"},"source":["The integer-encoding is arbitrary it does not capture any relationship between the words\n","\n","An integer-encoding can be challenging for a model to interpret\n","\n","for example,\n","A linear classifier learns a single weight for each feature because there is no relationship between the similarity of any two words and the similarity of their encodings, this feature-weight combination is not meaningful"]},{"cell_type":"markdown","metadata":{"id":"yDYWV_CKGUAn"},"source":["## 3) Word embeddings"]},{"cell_type":"markdown","metadata":{"id":"jnyEGJhxGdNo"},"source":["With the word embeddings is an efficient, dense representation in which similar words have a similar encoding\n","\n","In word embeddings we do not have to specify this encoding by hand\n","\n","**An embedding** is a dense vector of floating point values (the length of the vector is a parameter you specify)\n","\n","Instead of specifying values for the embedding manually, they are trainable parameters\n","\n","**trainable parameters:** weights learned by model during training, in the same way a model learns weights for a dense layer\n","\n","Word embeddings can have 8-dimensional (for small datasets), up to 1024-dimensions (for the large datasets)\n","\n","A higher dimensional embedding can capture fine-grained relationships between words, but takes more data to learn"]},{"cell_type":"markdown","metadata":{"id":"NFPDA_JeH8N0"},"source":["![alt text](https://drive.google.com/uc?id=15PinRU7Q9b6l7_YH2wKAf4yZFP8wLRzR)"]},{"cell_type":"markdown","metadata":{"id":"1gALp0CTIHvc"},"source":["In above word embedding diagram, each word is represented as a 4-dimensional vector of floating point values\n","\n","Another way to think of an embedding is as \"lookup table\"\n","\n","After these weights have been learned, we can encode each word by looking up the dense vector it corresponds to in the table"]}]}